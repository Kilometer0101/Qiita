# 1. 状態が持つ情報量と確率
ある状態Aが観測される。

この状態Aが珍しいものであればあるほど、その観測から得られる情報量は多い。

例えば下の2つの状態を比較します。

　状態1. 彼は中華人民共和国出身だ
　状態2. 彼はバチカン出身だ

「彼」について、より「情報量」が多いのはどちらでしょうか？
どちらも「出身国」という情報を示しているだけなので、両者がもたらす情報量は対等でしょうか。

しかし、中国の人口は13億7千万人[wiki](https://ja.wikipedia.org/wiki/%E4%B8%AD%E8%8F%AF%E4%BA%BA%E6%B0%91%E5%85%B1%E5%92%8C%E5%9B%BD)なのに対し、バチカンは千人以下[wiki](https://ja.wikipedia.org/wiki/%E3%83%90%E3%83%81%E3%82%AB%E3%83%B3)だそうです。

なので、

　状態1から得られる情報量：彼は約13億7千万人の中の誰かだ。
　状態2から得られる情報量：彼は約千人の中の誰かだ。

状態2が生じれば、「彼」を1000分の1の確率で特定できてしまう、という事です。
状態1では、そうはいきません。世界の7人に1人ぐらいは中国出身なので。

珍しい状態であればあるほど、それが持つ情報量が大きい、とはこういう理屈です。

# 2. 自己情報量 self information
[日wiki](https://ja.wikipedia.org/wiki/%E6%83%85%E5%A0%B1%E9%87%8F), [英wiki](https://en.wikipedia.org/wiki/Quantities_of_information)

上の議論から、状態$A$の持つ情報量$I(A)$は、その状態の生じる確率$p(A)$で表せる。
この$I(A)$をなんと呼ぶかというと、self informationと呼ぶ。
日本語表現はネット上では色々出てくる。
情報量、選択情報量、自己情報量、自己エントロピー.. etc

ここでは素直に自己情報量としておきます。

自己情報量$I(A)$には、いくつかの性質が求められます。
　1. 確率$p(A)$に対して単調減少
　2. 確率$p(A)$に対して連続
　3. 加法性を持つ

1に関しては上で議論しました。
2に関しては、僅かに$p(A)$が変化したら、$I(A)$も僅かに変化してほしい、という事。
3に関しては、状態$A$と状態$B$が独立に同時に得られた時、それらの自己情報量を単純加算したい、という事です。

「身長66cmである」「ジェダイ・マスターである」の自己情報量を加算したい。
個々の自己情報量では足りないけれど、2つを足せば「ヨーダ！？」に近くなる。

実は、この3つの性質を満たすという条件から関数は一義に決まってしまいます。

```math
I(A)=\log\frac{1}{p(A)}=-\log p(A)
```
この時、対数の底には何を取っても良いのですが、普通は2(bit)を使います。
10を取ればdigit, $e$を取ればnatという事ですね。
当然、底が違えば単純比較はできないので注意。

自己情報量$I(A)$の期待値をエントロピー(Shannon entropy)と呼び、$H(A)$と書きます。
期待値$E$は構成要素とそれが生じる確率の積和なので、

```math
\begin{eqnarray}
H(A)&=&E[I(A)]\\
&=&\sum_{i=1}^n p(a_i)\, I(a_i)\\
&=&-\sum_{i=1}^n p(a_i)\, \log p(a_i)
\end{eqnarray}
```

えーと、$a_i$は、$A$を構成する要素で、それぞれが生じる確率$p(a_i)$を考えるのでこうなるわけです。

```math
A = \{a_1,\ a_2,\ ...,\ a_i,\ ...,\ a_n\}
```

# 3. 情報量 divergence

で、何を考えるか、というとですね。

今、観測データ$x$が得られました。
このデータ$x$が確率変数$X$に従っている確率を$P(x)$とします。

```math
P(x)=p(X=x)
```

次に、新たな観測$a$が得られました。
この$a$を知った上で、観測$x$が確率変数$X$に従っている確率を$Q(x)$とします。

```math
Q(x)=p(X=x\,|\,a)
```

問題になっているのが$x$と$X$の関係にある事に注意してください。

新たな観測$a$によって得られた「$x$と$X$の関係に関する情報量」は、
$a$を得る前後の差分になるので、

```math
\big(-\log Q(x) \big)-\big(-\log P(x) \big)= \log \frac{P(x)}{Q(x)}
```

で、Shannon entropyと同様に$x$について期待値を取ったものをカルバック・ライブラー情報量 Kullback-Leibler divergenceと呼びます(連続分布の場合は積分)。

```math
D_{KL}(P||Q) = \sum_x p(x) \, \log \frac{P(x)}{Q(x)}
```

いや、いいんですよ、これでも。
これによって2つの確率分布$P(x)$と$Q(x)$を比較できますね。

ただですね、要注意ポイントは、PとQに対して対称じゃない事です。

```math
D_{KL}(P||Q) \neq D_{KL}(Q||P)
```

つまり$D_{KL}$は、$P(x)$と$Q(x)$の距離として扱えない。
不便だなぁ、という事で改良されたのがジェンセン・シャノン情報量 Jensen-Shannon divergenceです。

```math
\begin{eqnarray}
D_{JS}(P||Q)=&&λ \, D_{KL}\big(Q||λQ+(1-λ)P\big)\\
&&+(1-λ)\, D_{KL}\big(P||λQ+(1-λ)P\big)
\end{eqnarray}
```

対称となるのは、$λ=1/2$の時で、

```math
D_{JS}(P||Q)=\frac{1}{2}\bigg\{ D_{KL}(P \, || \, \frac{P}{2}+\frac{Q}{2})
+D_{KL}(Q \, || \, \frac{P}{2}+\frac{Q}{2}) \bigg\}=D_{JS}(Q||P)
```



# 実装
[前々回の記事](http://qiita.com/kilometer/items/58a1a353b9475bbf27c6)と[前回](http://qiita.com/kilometer/items/f410c27906ff073d2e91)に、これをRから使った計算例を出しています。

　

どっとはらい

